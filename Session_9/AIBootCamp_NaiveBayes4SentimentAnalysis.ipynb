{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marypthomas/ai-bootcamp-osu/blob/main/Session_9/AIBootCamp_NaiveBayes4SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTPwb8ny-hOU"
      },
      "source": [
        "# **AI Bootcamp: Naive Bayes**\n",
        "**Author**:\n",
        "- Prof. Eric Fosler-Lussier, The Ohio State University"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2T5S8IQTct"
      },
      "source": [
        "## Goals\n",
        "The goals of this notebook are to familarize you with:\n",
        "*   Naive Bayes\n",
        "*   Binary Classification\n",
        "*   Data exploration\n",
        "\n",
        "### **Part 0: Initial setup**\n",
        "\n",
        "**0.1:** While not completely necessary for this assignment, you may want to familiarize yourself with the following packages: [numpy](https://numpy.org), [scikit-learn](https://scikit-learn.org), [pandas](https://pandas.pydata.org), [matplotlib](https://matplotlib.org).\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Part 1: A Simple Bayes Net: Naive Bayes**\n",
        "\n",
        "In the lecture, we discussed how conditional independences of a joint probablity distribution get encoded by a Bayesian Network (BN). One of the simplest form of BNs is the Naive Bayes (NB) model which encodes a set of simple conditional independences: \n",
        "\n",
        "- Given a single cause all of the effects are independent from each other.\n",
        "- Mathematically: \n",
        "$P($*cause*$, $*effect*$_1, ..., $*effect*$_n) = P($*cause*$) \\prod_i P($*effect*$_i|$*cause*$)$ \n",
        "\n",
        "NB can be used for classification by assuming that cause is the true (unknown) label and it (probabilistically) generates all of the features (effects) while features are independent given the cause. \n",
        "\n",
        "For example, in sentiment analysis the *cause* is the author's sentiment (say, unknown label from the set of {sad, happy, feared, suprised, disgusted, angry}) and the *effects* are words that s/he writes. The simplifying assumption of NB says that knowing the latent sentiment, words of the sentence are independent. We know this assumption is not true because grammar and word-use impose some dependency structure between words in the sentence, but we choose to ignore that in this model.\n",
        "\n",
        "Although simple, NB has shown good performance in many classifcation tasks and has become a standard classic baseline for classification. \n",
        "\n",
        "Today we want to perform Twitter sentiment analysis using NB. The goal is to figure out if a tweet has a positive or negative sentiment about the weather.  \n",
        "\n",
        "**1.0:** Set up the environment (you can click on the play button below to import the appropriate modules)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6fYsm5f-UbI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-MaOhV5UUvD"
      },
      "source": [
        "**1.1** Read the data from GitHub into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HskImt85UhU-"
      },
      "outputs": [],
      "source": [
        "TweetUrl='https://github.com/efosler/cse5522data/raw/master/db3_final_clean.csv'\n",
        "tweet_dataframe=pd.read_csv(TweetUrl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvgt0PYLVHYr"
      },
      "source": [
        "**1.2** Print out the top of the dataframe to make sure that the data loaded correctly.  It should be a data table with three columns (weight, tweet, label), and 3697 rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScERznWSVBK3"
      },
      "outputs": [],
      "source": [
        "display(tweet_dataframe.shape)\n",
        "tweet_dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLwuLvFQuvGU"
      },
      "source": [
        "Labels are -1 and +1 for negative and positive sentiments respectively. Multiple judges have been asked to choose a label for a tweet (this is an example of crowd-sourcing) from five possible labels: \n",
        "\n",
        "- Tweet is not relevant to weather. \n",
        "- I can't tell the sentiment. \n",
        "- Neutral: author just sharing information. \n",
        "- Positive\n",
        "- Negative\n",
        "\n",
        "The majority vote was picked as the label and its ratio was set as the weight of the tweet. So for the tweet in row 2 above, 61% of judges voted that the label is negative.\n",
        "\n",
        "Note that tweets have been pre-processed (or cleaned). For example, :) and :( :) were replaced with \"sad\" and \"smiley\" and numbers with \"num\", etc. You can go further (as we ask in 1.12) and remove the stop words, i.e., repetitive non-informative words such as am, is, and are. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AssaBgdR0K"
      },
      "source": [
        "**1.3.** In the next step, we should build our feature matrix by converting the string of words to a vector of numeric values. \n",
        "\n",
        "First we need to assign a unique id to each word and create the feature matrix with correct size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Q7tGhlVcOR"
      },
      "outputs": [],
      "source": [
        "# wordDict maps words to id\n",
        "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
        "wordDict = {}\n",
        "idCounter = 0\n",
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    if word not in wordDict:\n",
        "      wordDict[word] = idCounter\n",
        "      idCounter += 1\n",
        "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JkM_oBZ7cv7"
      },
      "source": [
        "Checking head of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdK4g_D8hX-4"
      },
      "outputs": [],
      "source": [
        "dict(list(wordDict.items())[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmXPr6qeSQo"
      },
      "source": [
        "**1.4:** The simplest way of coding a tweet to numbers is to mark the occurrence of a word, and forget about its frequency in the document (tweet). This works well with tweets as there are not many repetitive words in a single tweet. So let's fill the document-word matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap5o8fzI7rgQ"
      },
      "outputs": [],
      "source": [
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    X[i, wordDict[word]]  = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "714iX2JA9XMh"
      },
      "source": [
        "Now we check if the number of words are correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aiap5wBW86lZ"
      },
      "outputs": [],
      "source": [
        "np.sum(X[0:5, ], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmPNK1Su-Hwf"
      },
      "source": [
        "Finally, we extract the labels from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGm_x8Nm-HL6"
      },
      "outputs": [],
      "source": [
        "y = np.array(tweet_dataframe.iloc[:,2])\n",
        "y[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ-EgzBo-wLd"
      },
      "source": [
        "Let's compute the total number of positive and negative tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFKKNsM7-_UN"
      },
      "outputs": [],
      "source": [
        "numNeg = np.where(y > 0)[0][0] - 1\n",
        "numPos = len(y) - numNeg\n",
        "probNeg = numNeg / (numNeg + numPos)\n",
        "probPos = 1 - probNeg\n",
        "display(numNeg, numPos, probNeg, probPos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlOIWDiI7JQw"
      },
      "source": [
        "So samples 0:1649 are negative and 1650:-1 are positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-s3N0xJOILK"
      },
      "source": [
        "**1.5: Train/Test Split** Now with do the 20/80 split and learn the word probabilities using the 80 % part and test the NB performance on the 20 % part. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbdLXcY0PCQX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "display(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2LTz2CP95GT"
      },
      "source": [
        "**1.6: Computing Probabilities by Counting** Now the real work begins. Write the code that, from the train feature matrix xTrain computes the needed word probabilites, i.e., $P(word|label)$ where label is + or - and word is any of the words saved in the `wordDict`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ7KjLPk7oMZ"
      },
      "outputs": [],
      "source": [
        "# compute three distributions (four variables):\n",
        "#\n",
        "# probWordGivenPositive: P(word|Sentiment = +ive)\n",
        "# probWordGivenNegative: P(word|Sentiment = -ive)\n",
        "# priorPositive: P(Sentiment = +ive)\n",
        "# priorNegative: P(Sentiment = -ive)\n",
        "#  (note these last two form one distribution)\n",
        "\n",
        "# compute distributions here - to compute P(word|Sentiment = +ive), select all positive tweets,\n",
        "#     then count the number of times each word occurs.  Normalize by total number of words in positive tweets.\n",
        "# do the same thing for negative tweets.\n",
        "\n",
        "\n",
        "# checking the results\n",
        "display(probWordGivenPositive[0:5])\n",
        "display(probWordGivenNegative[0:5])\n",
        "display(priorPositive, priorNegative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUFU8eUQ_8gC"
      },
      "source": [
        "Note that you only needed to compute $P(word = 1| +)$ or $P(word = 1| -)$ and the probabilities of the word being absent from a tweet is just 1 minus those probabilities. \n",
        "\n",
        "My answer is hidden below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "CM_g4i9TEUb8"
      },
      "outputs": [],
      "source": [
        "# set up count vectors fpr\n",
        "probWordGivenPositive = np.zeros(len(wordDict), dtype='float') #X.shape[1] == len(wordDict)\n",
        "probWordGivenNegative = np.zeros(len(wordDict), dtype='float')\n",
        "priorPositive = 0\n",
        "priorNegative = 0\n",
        "\n",
        "# note that this works because xTrain only has 1 if the word is present, not the count.\n",
        "probWordGivenPositive = sum(xTrain[yTrain == +1,]) / sum(yTrain == +1)\n",
        "probWordGivenNegative = sum(xTrain[yTrain == -1,]) / sum(yTrain == -1)\n",
        "\n",
        "#### EDIT FOR NEXT SECTION:\n",
        "#### note that the if probWordGivenPositive has a zero prob, then the log(prob) is undefined\n",
        "#### so we can fix this by adding a small epsilon to both Count(word=1|+ive) and Count(word=0|-ive)\n",
        "####\n",
        "#### this idea is called Laplace smoothing\n",
        "epsilon=0.001\n",
        "probWordGivenPositive = (sum(xTrain[yTrain == +1,])+epsilon) / (sum(yTrain == +1)+2*epsilon)\n",
        "probWordGivenNegative = (sum(xTrain[yTrain == -1,])+epsilon) / (sum(yTrain == -1)+2*epsilon)\n",
        "\n",
        "priorNegative = sum(yTrain == -1) / len(yTrain)\n",
        "priorPositive = sum(yTrain == +1) / len(yTrain)\n",
        "\n",
        "# checking the results\n",
        "display(probWordGivenPositive[0:5])\n",
        "display(probWordGivenNegative[0:5])\n",
        "display(priorPositive, priorNegative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnNcCRQgEUb8"
      },
      "source": [
        "However, as we see in 1.6, for convenience, we will also want to compute $log P(word = 1 | +)$, $log P(word = 0 | +)$, $log P(word = 1 | -)$ and $log P(word = 0 | -)$.  Also we should compute the log priors.  Let's do so now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HLcaaDTiwF0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# compute the following:\n",
        "# logProbWordPresentGivenPositive\n",
        "# logProbWordAbsentGivenPositive\n",
        "# logProbWordPresentGivenNegative\n",
        "# logProbWordAbsentGivenNegative\n",
        "# logPriorPositive\n",
        "# logPriorNegative\n",
        "\n",
        "\n",
        "# Did this work, or did you get an error?  (Read below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXVQ7ZHAkH1u"
      },
      "source": [
        "You likely received an error when you tried to take $log(0)$ at some point.  Can your group think of a way to avoid taking $log(0)$?  Check in with your instructor/TA to see if what you're thinking will work.  Implement that change in your code above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "ZPfU1uxGEUb8"
      },
      "outputs": [],
      "source": [
        "logProbWordPresentGivenPositive = np.log(probWordGivenPositive)\n",
        "logProbWordAbsentGivenPositive = np.log(1-probWordGivenPositive)\n",
        "logProbWordPresentGivenNegative = np.log(probWordGivenNegative)\n",
        "logProbWordAbsentGivenNegative = np.log(1-probWordGivenNegative)\n",
        "logPriorPositive = np.log(priorPositive)\n",
        "logPriorNegative = np.log(priorNegative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxdbYsu9E8av"
      },
      "source": [
        "**1.6: Math of NB** Here we provide the derivation of NB when we want to classify the $i$th tweet $\\textbf{x}^{(i)}$ and the size of dictionary is $p$, i.e., each tweet is a binary vector of size $p$ as $\\textbf{x}^{(i)} = (x_1^{(i)},\\dots, x_p^{(i)})$. \n",
        "\n",
        "Note that we computed $P(x_j^{(i)} = 1|+)$ and $P(x_j^{(i)} = 1|-)$ in above code from `xTrain` and now want to classify `xTest` samples.\n",
        "\n",
        "**Classification Rule:** For each tweet $i$ NB classifier assigns label + if $P(+|\\textbf{x}^{(i)}) > P(-|\\textbf{x}^{(i)})$ and negative otherwise. \n",
        "\n",
        "These posterior probabilities can be computed using prior probabilities (that we got from `xTrain`) and Bayes rule as follows: \n",
        "\n",
        "\\begin{align}\n",
        "P(+|\\textbf{x}^{(i)}) &= \\alpha P(\\{\\textbf{x}^{(i)}\\}_{i=1}^n | +)P(+) \n",
        "\\\\\n",
        "(\\text{NB Assumption}) &= \\alpha P(+) \\prod_{j=1}^p P(x_j^{(i)}|+)\n",
        "\\end{align}\n",
        "\n",
        "For computational convinence (preventing underflow while dealing with small numbers) we work with the $\\log$ of probabilities:\n",
        "\n",
        "\\begin{align} \n",
        "\\log(P(+|\\textbf{x}^{(i)})) &\\propto \\log P(+) + \\sum_{j=1}^p \\log P(x_j^{(i)}|+) \n",
        "\\\\\n",
        "\\log(P(-|\\textbf{x}^{(i)})) &\\propto \\log P(-) + \\sum_{j=1}^p \\log P(x_j^{(i)}|-) \n",
        "\\end{align} \n",
        "\n",
        "Finally we can compute the confidence of our prediction as the log of the ratio of posteriors: \n",
        "$\\log(\\frac{P(\\text{predicted label}|\\textbf{x}^{(i)})}{P(\\text{the other label}|\\textbf{x}^{(i)})})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1AvG-LXTmPJ"
      },
      "source": [
        "**1.7: Implementing NB** Now write a function that takes a row of `xTest` and output a label for it based on NB classification rule. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu3YKPlzeFLb"
      },
      "outputs": [],
      "source": [
        "# classifyNB: \n",
        "#   words - vector of words of the tweet (binary vector)\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns (label of x according to the NB classification rule, confidence about the label)\n",
        "\n",
        "# Note: you can also change the function definition if you wish to encapsulate all six log probs\n",
        "# as one model; just make sure to follow through below\n",
        "\n",
        "def classifyNB(words,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "               logPriorPositive, logPriorNegative):\n",
        "  # fill in function definition here\n",
        "  \n",
        "print(classifyNB(xTest[700, ], logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "               logPriorPositive, logPriorNegative)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "AfgtlTFzEUb9"
      },
      "outputs": [],
      "source": [
        "def classifyNB(words,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "               logPriorPositive, logPriorNegative):\n",
        "    positivePosterior=logPriorPositive + sum(logProbWordPresentGivenPositive[words==1]) + sum(logProbWordAbsentGivenPositive[words==0])\n",
        "    negativePosterior=logPriorNegative + sum(logProbWordPresentGivenNegative[words==1]) + sum(logProbWordAbsentGivenNegative[words==0])\n",
        "\n",
        "    if (positivePosterior > negativePosterior):\n",
        "        return (+1, positivePosterior-negativePosterior)\n",
        "    else:\n",
        "        return (-1, negativePosterior-positivePosterior)\n",
        "\n",
        "print((classifyNB(xTest[700, ], logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "               logPriorPositive, logPriorNegative)),yTest[700])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7F8osLy3ia"
      },
      "source": [
        "**1.8:** Compute the output of `classifyNB` for all test data and output the average error.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_J3BdyCfCVL"
      },
      "outputs": [],
      "source": [
        "# testNB: Classify all xTest\n",
        "#   xTest - test data features\n",
        "#   yTest - true label of test data\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns Average test error\n",
        "def testNB(xTest, yTest, \n",
        "           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "           logPriorPositive, logPriorNegative):\n",
        "    correct=0\n",
        "    for i in range(len(yTest)):\n",
        "        (y,diff)=classifyNB(xTest[i, ], logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "                            logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "                            logPriorPositive, logPriorNegative)\n",
        "        if y==yTest[i]:\n",
        "            correct=correct+1\n",
        "            \n",
        "    avgErr=1-(float(correct)/float(len(yTest)))\n",
        "  \n",
        "    print(\"Average error of NB is\", avgErr)\n",
        "    return avgErr\n",
        "\n",
        "testNB(xTest, yTest, \n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
        "       logPriorPositive, logPriorNegative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI_h3yxKMe2O"
      },
      "source": [
        "**1.9: Ignoring absent words** The basic formalism asks you to take account of both present and absent words.  What happens if you ignore the absent words?  Does prediction change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MBglC8xEUb9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "SESSION_III",
      "language": "python",
      "name": "venv_session_iii"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}